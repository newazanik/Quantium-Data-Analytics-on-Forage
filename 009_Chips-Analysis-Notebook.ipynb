# Chips Analysis Notebook (Jupyter-friendly .py with cell markers)
# Save this file as `Chips_analysis_notebook.py` or use it with jupytext to convert to .ipynb
# Usage: open in VSCode / JupyterLab or convert with jupytext: jupytext --to ipynb Chips_analysis_notebook.py

# %% [markdown]
# # Chips Category — Data Preparation & Customer Analytics
#
# This notebook performs the end-to-end data preparation and initial customer analytics required for the Quantium Chips category task.
# It:
# - Loads provided files: `QVI_transaction_data.xlsx` (sheet: "in") and `QVI_purchase_behaviour.csv`
# - Converts Excel serial dates to datetimes
# - Cleans and validates numeric and key identifier fields
# - Derives features: `price`, `pack_size`, `brand_guess`, `is_chips`
# - Filters to chips-only rows and removes problematic records
# - Computes customer-level RFM and behavioural metrics
# - Produces aggregated CSVs and PNG visualisations for the report
#
# Save & run the whole notebook. When finished, export to PDF for Zilinka (File -> Export Notebook As -> PDF).

# %%
# Environment setup: imports
import pandas as pd
import numpy as np
import re
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt

# set display options for nicer outputs in notebook
pd.options.display.max_columns = 120
pd.options.display.width = 140

# %% [markdown]
# ## File paths — adjust if you placed files in another folder

# %%
DATA_DIR = Path('.')  # set if files are in a subfolder
TXN_XLSX = DATA_DIR / 'QVI_transaction_data.xlsx'
CUST_CSV = DATA_DIR / 'QVI_purchase_behaviour.csv'
OUT_DIR = DATA_DIR / 'outputs'
OUT_DIR.mkdir(parents=True, exist_ok=True)

print('Paths:')
print(' TXN_XLSX ->', TXN_XLSX)
print(' CUST_CSV ->', CUST_CSV)
print(' OUT_DIR ->', OUT_DIR)

# %% [markdown]
# ## Utility functions

# %%

def excel_serial_to_date(x):
    """Convert Excel serial (e.g., 43390) to pandas datetime. Fallbacks to pd.to_datetime."""
    try:
        if pd.isna(x):
            return pd.NaT
        xi = float(x)
        # integer part as days (Excel uses 1899-12-30 as epoch in many exports)
        return pd.to_datetime('1899-12-30') + pd.to_timedelta(int(xi), unit='D')
    except Exception:
        try:
            return pd.to_datetime(x, errors='coerce', dayfirst=True)
        except Exception:
            return pd.NaT


def extract_pack_size(text):
    if pd.isna(text):
        return np.nan
    s = str(text).lower()
    # capture patterns like '175g', '210G', '1kg', '6x40g'
    m = re.search(r"(\d+\s?x\s?\d+\s?g|\d+\s?g|\d+\s?kg|\d+\s?ml|\d+\s?l)", s)
    if m:
        return re.sub(r"\s+", "", m.group(0))
    # some descriptors: single, family, multipack
    if 'single' in s:
        return 'single'
    if 'family' in s:
        return 'family'
    if 'multipack' in s or 'multi' in s:
        return 'multipack'
    return np.nan


def extract_brand(text):
    if pd.isna(text):
        return np.nan
    tokens = re.split(r'[\s\-\(\)\,\/]+', str(text))
    if len(tokens) > 0:
        brand = tokens[0].strip()
        # avoid returning a numeric token like '175g'
        if re.match(r'^\d', brand) and len(tokens) > 1:
            brand = tokens[1].strip()
        return brand.title()
    return np.nan

# %% [markdown]
# ## Load data

# %%
# Load Excel transactions
print('Reading transaction Excel...')
xls = pd.ExcelFile(TXN_XLSX)
print('Sheets:', xls.sheet_names)
# we expect sheet 'in'
sheet_name = 'in' if 'in' in xls.sheet_names else xls.sheet_names[0]
raw_tx = pd.read_excel(xls, sheet_name=sheet_name, dtype=object)
print('Transaction raw shape:', raw_tx.shape)

# Load customer demographics
print('Reading customer file...')
cust = pd.read_csv(CUST_CSV, low_memory=False)
print('Customer raw shape:', cust.shape)

# %% [markdown]
# ## Inspect and standardize columns

# %%
print('Transaction columns (sample):', raw_tx.columns.tolist())
print('Customer columns (sample):', cust.columns.tolist())

# lower-case & friendly names
raw_tx.columns = [c.strip().lower().replace(' ', '_') for c in raw_tx.columns]
raw_tx.rename(columns={
    'lylty_card_nbr':'customer_id',
    'prod_name':'product_description',
    'prod_qty':'quantity',
    'tot_sales':'line_total',
    'txn_id':'transaction_id',
    'prod_nbr':'sku',
    'store_nbr':'store_id'
}, inplace=True)

cust.columns = [c.strip().lower().replace(' ', '_') for c in cust.columns]
# unify column name for customer id in cust
if 'lylty_card_nbr' in cust.columns:
    cust.rename(columns={'lylty_card_nbr':'customer_id'}, inplace=True)

print('Standardized tx columns:', raw_tx.columns.tolist())
print('Standardized cust columns:', cust.columns.tolist())

# %% [markdown]
# ## Convert date column

# %%
# Some exports store Excel serials in a `date` column; convert them
if 'date' in raw_tx.columns:
    raw_tx['date'] = raw_tx['date'].apply(excel_serial_to_date)
else:
    # fallback: try first column if it holds date
    raw_tx.insert(0, 'date', raw_tx.iloc[:,0].apply(excel_serial_to_date))

print('Date range after conversion:', raw_tx['date'].min(), 'to', raw_tx['date'].max())

# %% [markdown]
# ## Numeric conversions and basic checks

# %%
raw_tx['quantity'] = pd.to_numeric(raw_tx['quantity'], errors='coerce')
raw_tx['line_total'] = pd.to_numeric(raw_tx['line_total'], errors='coerce')
raw_tx['price'] = raw_tx['line_total'] / raw_tx['quantity']

# quick null summary
print('Nulls in transactions (top 10):')
print(raw_tx.isnull().sum().sort_values(ascending=False).head(10))

# Check for non-positive price/quantity
print('Non-positive quantities:', (raw_tx['quantity'] <= 0).sum(), 'Non-positive/NaN prices:', (raw_tx['price'] <= 0).sum())

# %% [markdown]
# ## Feature extraction: pack_size & brand

# %%
raw_tx['pack_size'] = raw_tx['product_description'].apply(extract_pack_size)
raw_tx['brand_guess'] = raw_tx['product_description'].apply(extract_brand)

print('Sample product descriptions and derived features:')
display(raw_tx[['product_description','pack_size','brand_guess']].head(10))

# %% [markdown]
# ## Merge customer demographics

# %%
merged = raw_tx.merge(cust, how='left', on='customer_id')
print('Merged shape:', merged.shape)
print('Missing customer_id after merge:', merged['customer_id'].isnull().sum())

# %% [markdown]
# ## Identify chips rows

# %%
merged['is_chips'] = merged['product_description'].astype(str).str.lower().str.contains('chip|crisp|potato|dorito|doritos', na=False)
print('Total rows flagged as chips:', merged['is_chips'].sum(), 'out of', len(merged))

# %% [markdown]
# ## Filter chips and clean problematic rows

# %%
chips = merged[merged['is_chips']].copy()
before = len(chips)
chips = chips[(chips['quantity'] > 0) & (chips['price'] > 0) & chips['transaction_id'].notnull() & chips['customer_id'].notnull()]
after = len(chips)
print(f'Removed {before-after} problematic rows; remaining chips rows: {after}')

# Save a flagged file for audit
flagged = merged[~((merged['quantity'] > 0) & (merged['price'] > 0) & merged['transaction_id'].notnull() & merged['customer_id'].notnull())]
flagged.to_csv(OUT_DIR / 'transactions_flagged.csv', index=False)

# Persist chips cleaned dataset
chips.to_csv(OUT_DIR / 'tx_chips_clean.csv', index=False)
print('Saved tx_chips_clean.csv')

# %% [markdown]
# ## Customer-level metrics (RFM + behaviour)

# %%
snapshot_date = chips['date'].max() + pd.Timedelta(days=1)
rfm = chips.groupby('customer_id').agg(
    recency_days = ('date', lambda x: int((snapshot_date - x.max()).days)),
    frequency = ('transaction_id', 'nunique'),
    monetary = ('line_total', 'sum'),
    units = ('quantity', 'sum'),
    avg_units_per_tx = ('quantity', lambda s: float(s.sum())/s.nunique())
).reset_index()

# Quintile-based RFM scores
rfm['r_score'] = pd.qcut(rfm['recency_days'].rank(method='first'), 5, labels=[5,4,3,2,1]).astype(int)
rfm['f_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)
rfm['m_score'] = pd.qcut(rfm['monetary'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)
rfm['rfm_score'] = rfm['r_score'].astype(str) + rfm['f_score'].astype(str) + rfm['m_score'].astype(str)

rfm.to_csv(OUT_DIR / 'chips_customers_rfm.csv', index=False)
print('Saved chips_customers_rfm.csv')

# %% [markdown]
# ## Aggregations for reporting

# %%
pack_agg = chips.groupby('pack_size').agg(units_sold=('quantity','sum'), revenue=('line_total','sum'), transactions=('transaction_id','nunique')).reset_index().sort_values('units_sold', ascending=False)
brand_agg = chips.groupby('brand_guess').agg(units_sold=('quantity','sum'), revenue=('line_total','sum')).reset_index().sort_values('units_sold', ascending=False)

pack_agg.to_csv(OUT_DIR / 'chips_sales_by_pack_size.csv', index=False)
brand_agg.to_csv(OUT_DIR / 'chips_sales_by_brand_guess.csv', index=False)
print('Saved pack and brand aggregations')

# %% [markdown]
# ## Visualisations (save PNGs)

# %%
daily = chips.groupby(chips['date'].dt.date)['line_total'].sum().reset_index()
daily['date'] = pd.to_datetime(daily['date'])
plt.figure(figsize=(10,4))
plt.plot(daily['date'], daily['line_total'])
plt.title('Daily Chips Revenue')
plt.ylabel('Revenue')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(OUT_DIR / 'fig_daily_chips_revenue.png')
plt.close()

# Pack size bar (top 12)
if pack_agg['units_sold'].notnull().sum() > 0:
    sample = pack_agg.head(12).set_index('pack_size')
    plt.figure(figsize=(8,4))
    sample['units_sold'].plot(kind='bar')
    plt.title('Top Pack Sizes by Units Sold')
    plt.tight_layout()
    plt.savefig(OUT_DIR / 'fig_pack_size_units.png')
    plt.close()

print('Saved figures to', OUT_DIR)

# %% [markdown]
# ## Quick summary numbers (print to notebook)

# %%
print('Total transaction rows:', len(raw_tx))
print('Total chips rows after cleaning:', len(chips))
print('Date range in data:', raw_tx['date'].min(), 'to', raw_tx['date'].max())
print('Unique chip customers:', rfm['customer_id'].nunique())

# show top 10 pack sizes and brands in the notebook
print('\nTop pack sizes:')
display(pack_agg.head(10))
print('\nTop brand guesses:')
display(brand_agg.head(10))

# %% [markdown]
# ## Next steps (suggested cells to add after this notebook)
# 1. RFM segment profiling: merge `rfm` with `cust` demographics and compute segment summaries
# 2. KMeans clustering on behavioural features (frequency, monetary, avg_units_per_tx)
# 3. Basket analysis: co-occurrence of chips with other categories for cross-sell
# 4. Promo and price-lift analysis (if promo flags / baseline prices exist)
# 5. Build PowerPoint slides by importing the saved PNGs and CSVs

# %% [markdown]
# ### End of notebook
# Save the notebook as .ipynb via jupytext or run in Jupyter and export to PDF for Zilinka.
